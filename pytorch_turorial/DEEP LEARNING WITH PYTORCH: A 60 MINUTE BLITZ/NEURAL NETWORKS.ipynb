{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=400, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # 1 channel (32x32) image => (28x28) image, so (5x5) kernel with stride 1\n",
    "        \n",
    "        # conv layer\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5) # (# input channel, # output channel, kernel size)\n",
    "        self.conv2 = nn.Conv2d(6, 16 ,5) # (# input channel, # output channel, kernel size)\n",
    "        \n",
    "        # fc layer\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120) # 16 * (5x5) feature map => 120 feature\n",
    "        self.fc2 = nn.Linear(120, 84) # 120 feature => 84 feature\n",
    "        self.fc3 = nn.Linear(84, 10) # 84 feature => 10 feature\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # max pooling over (2x2) window\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        \n",
    "        # if size is square, can specify only single number\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:] # all dimension except batch dimension, (# batch, # channel, width, height)\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "            \n",
    "        return num_features\n",
    "    \n",
    "    \n",
    "net = Net()\n",
    "print(net)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'generator'>\n",
      "10\n",
      "torch.Size([6, 1, 5, 5])\n",
      "torch.Size([6])\n",
      "torch.Size([16, 6, 5, 5])\n",
      "torch.Size([16])\n",
      "torch.Size([120, 400])\n",
      "torch.Size([120])\n",
      "torch.Size([84, 120])\n",
      "torch.Size([84])\n",
      "torch.Size([10, 84])\n",
      "torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "params = net.parameters()\n",
    "print(type(params))\n",
    "params = list(net.parameters())\n",
    "print(len(params))\n",
    "\n",
    "# weight1 -> bias1 -> weight2 -> bias2 ...\n",
    "for param in params:\n",
    "    print(param.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0545,  0.0282, -0.0131, -0.0900,  0.0643, -0.0489, -0.1211, -0.0220,\n",
      "          0.0320, -0.0396]], grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "# test with (32x32) random input\n",
    "input = torch.randn(1, 1, 32, 32) # (# batch, # channel, weight, height)\n",
    "out = net(input)\n",
    "print(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zero the gradient buffers of all parameters and backprops with random gradients:\n",
    "net.zero_grad()\n",
    "out.backward(torch.randn(1, 10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntorch.nn은 mini-batch만 취급함, single input은 취급 안함 => 1개짜리의 경우 1개 짜리 mini-batch의 shape를 만들어줘야함\\nex) conv2d will take 4D tensor (batch number, # input channels, height, width)\\nIf you have a single sample, just use input.unsqueeze(0) to add a fake batch dimension.\\n'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "torch.nn은 mini-batch만 취급함, single input은 취급 안함 => 1개짜리의 경우 1개 짜리 mini-batch의 shape를 만들어줘야함\n",
    "ex) conv2d will take 4D tensor (batch number, # input channels, height, width)\n",
    "If you have a single sample, just use input.unsqueeze(0) to add a fake batch dimension.\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6187, grad_fn=<MseLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "# apply loss\n",
    "output = net(input)\n",
    "target = torch.randn(10) # dummy target\n",
    "target = target.view(1, -1) # output과 같은 shape로 만들어줌\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "loss = criterion(output, target)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nSo, when we call loss.backward(), the whole graph is differentiated w.r.t. the loss, \\nand all Tensors in the graph that has requires_grad=True will have their .grad Tensor \\naccumulated with the gradient.\\n'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "So, when we call loss.backward(), the whole graph is differentiated w.r.t. the loss, \n",
    "and all Tensors in the graph that has requires_grad=True will have their .grad Tensor \n",
    "accumulated with the gradient.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<MseLossBackward object at 0x113181470>\n",
      "<AddmmBackward object at 0x1131814e0>\n",
      "<AccumulateGrad object at 0x113181470>\n"
     ]
    }
   ],
   "source": [
    "print(loss.grad_fn) # MSE Loss\n",
    "print(loss.grad_fn.next_functions[0][0]) # Linear\n",
    "print(loss.grad_fn.next_functions[0][0].next_functions[0][0]) # ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.bias.grad before backward\n",
      "tensor([0., 0., 0., 0., 0., 0.])\n",
      "conv1.bias.grad after backward\n",
      "tensor([ 1.0062e-02, -9.0888e-03, -2.1298e-03,  6.0976e-03,  6.6808e-05,\n",
      "        -2.5223e-03])\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "backprop\n",
    "\n",
    "To backpropagate the error all we have to do is to loss.backward(). \n",
    "You need to clear the existing gradients though, else gradients will be accumulated to existing gradients.\n",
    "'''\n",
    "\n",
    "net.zero_grad() # zeroes the gradient buffers of all parameters\n",
    "print('conv1.bias.grad before backward')\n",
    "print(net.conv1.bias.grad)\n",
    "\n",
    "loss.backward()\n",
    "\n",
    "print('conv1.bias.grad after backward')\n",
    "print(net.conv1.bias.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before update Parameter containing:\n",
      "tensor([ 0.1585, -0.0590,  0.0572,  0.0071, -0.1634, -0.1086],\n",
      "       requires_grad=True)\n",
      "after update Parameter containing:\n",
      "tensor([ 0.1584, -0.0589,  0.0572,  0.0071, -0.1634, -0.1086],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "The simplest update rule used in practice is the Stochastic Gradient Descent (SGD):\n",
    "\n",
    "weight = weight - learning_rate * gradient\n",
    "However, as you use neural networks, you want to use various different update rules such as SGD, \n",
    "Nesterov-SGD, Adam, RMSProp, etc. To enable this, we built a small package: \n",
    "torch.optim that implements all these methods. Using it is very simple:\n",
    "'''\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "# create optimizer\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.01)\n",
    "\n",
    "# training loop\n",
    "optimizer.zero_grad() # zero gradient buffer\n",
    "output = net(input)\n",
    "loss = criterion(output, target)\n",
    "\n",
    "print('before update', net.conv1.bias)\n",
    "\n",
    "loss.backward()\n",
    "optimizer.step() # update\n",
    "\n",
    "print('after update', net.conv1.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
