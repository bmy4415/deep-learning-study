{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Assignment1-3_NN_with_TF.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"Z1jSL9fU_6UH","colab_type":"text"},"cell_type":"markdown","source":["# M2177.003100 Deep Learning <br> Assignment #1 Part 3: Playing with Neural Networks by TensorFlow"]},{"metadata":{"id":"g2eWDkYx_6UI","colab_type":"text"},"cell_type":"markdown","source":["Copyright (C) Data Science & AI Laboratory, Seoul National University. This material is for educational uses only. Some contents are based on the material provided by other paper/book authors and may be copyrighted by them. "]},{"metadata":{"colab_type":"text","id":"kR-4eNdK6lYS"},"cell_type":"markdown","source":["Previously in `Assignment2-1_Data_Curation.ipynb`, we created a pickle with formatted datasets for training, development and testing on the [notMNIST dataset](http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html).\n","\n","The goal of this assignment is to progressively train deeper and more accurate models using TensorFlow.\n","\n","**Note**: certain details are missing or ambiguous on purpose, in order to test your knowledge on the related materials. However, if you really feel that something essential is missing and cannot proceed to the next step, then contact the teaching staff with clear description of your problem.\n","\n","### Submitting your work:\n","<font color=red>**DO NOT clear the final outputs**</font> so that TAs can grade both your code and results.  \n","Once you have done **part 1 - 3**, run the *CollectSubmission.sh* script with your **Student number** as input argument. <br>\n","This will produce a compressed file called *[Your student number].tar.gz*. Please submit this file on ETL. &nbsp;&nbsp; (Usage: ./*CollectSubmission.sh* &nbsp; 20\\*\\*-\\*\\*\\*\\*\\*)"]},{"metadata":{"id":"S8MOPEVIAHEz","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":2688},"outputId":"2d0a31ce-f551-4251-b033-f125d8b0821d","executionInfo":{"status":"ok","timestamp":1543875403335,"user_tz":-540,"elapsed":343091,"user":{"displayName":"Seol JaeWan","photoUrl":"","userId":"07332071038155379708"}}},"cell_type":"code","source":["## load modules in google colab environment\n","## run this cell only when using google colab\n","!pip install termcolor\n","from termcolor import colored\n","\n","## print command in different color\n","def print_terminal_command(command) :\n","  print(colored('@ ' + command, 'green'))\n","  \n","\n","print_terminal_command('pwd')\n","!pwd\n","print_terminal_command('ls -al')\n","!ls -al\n","print_terminal_command('mkdir -p data')\n","!mkdir -p data\n","print_terminal_command('ls -al')\n","!ls -al\n","\n","# These are all the modules we'll be using later. Make sure you can import them\n","# before proceeding further.\n","from __future__ import print_function\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import os\n","import sys\n","import tarfile\n","from IPython.display import display, Image\n","from scipy import ndimage\n","from sklearn.linear_model import LogisticRegression\n","from six.moves.urllib.request import urlretrieve\n","from six.moves import cPickle as pickle\n","\n","# Config the matplotlib backend as plotting inline in IPython\n","%matplotlib inline\n","# PLEASE Comment this line on submission\n","\n","\n","url = 'https://commondatastorage.googleapis.com/books1000/'\n","last_percent_reported = None\n","data_root = './data' # Change me to store data elsewhere\n","\n","def download_progress_hook(count, blockSize, totalSize):\n","    \"\"\"A hook to report the progress of a download. This is mostly intended for users with\n","    slow internet connections. Reports every 5% change in download progress.\n","    \"\"\"\n","    global last_percent_reported\n","    percent = int(count * blockSize * 100 / totalSize)\n","\n","    if last_percent_reported != percent:\n","        if percent % 5 == 0:\n","            sys.stdout.write(\"%s%%\" % percent)\n","            sys.stdout.flush()\n","        else:\n","            sys.stdout.write(\".\")\n","            sys.stdout.flush()\n","\n","    last_percent_reported = percent\n","        \n","def maybe_download(filename, expected_bytes, force=False):\n","    \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n","    dest_filename = os.path.join(data_root, filename)\n","    if force or not os.path.exists(dest_filename):\n","        print('Attempting to download:', filename) \n","        filename, _ = urlretrieve(url + filename, dest_filename, reporthook=download_progress_hook)\n","        print('\\nDownload Complete!')\n","    statinfo = os.stat(dest_filename)\n","    if statinfo.st_size == expected_bytes:\n","        print('Found and verified', dest_filename)\n","    else:\n","        raise Exception(\n","          'Failed to verify ' + dest_filename + '. Can you get to it with a browser?')\n","    return dest_filename\n","\n","train_filename = maybe_download('notMNIST_large.tar.gz', 247336696)\n","test_filename = maybe_download('notMNIST_small.tar.gz', 8458043)\n","\n","num_classes = 10\n","np.random.seed(133)\n","\n","def maybe_extract(filename, force=False):\n","    root = os.path.splitext(os.path.splitext(filename)[0])[0]  # remove .tar.gz\n","    if os.path.isdir(root) and not force:\n","    # You may override by setting force=True.\n","        print('%s already present - Skipping extraction of %s.' % (root, filename))\n","    else:\n","        print('Extracting data for %s. This may take a while. Please wait.' % root)\n","        tar = tarfile.open(filename)\n","        sys.stdout.flush()\n","        tar.extractall(data_root)\n","        tar.close()\n","    data_folders = [\n","        os.path.join(root, d) for d in sorted(os.listdir(root))\n","        if os.path.isdir(os.path.join(root, d))]\n","    if len(data_folders) != num_classes:\n","        raise Exception(\n","          'Expected %d folders, one per class. Found %d instead.' % (\n","            num_classes, len(data_folders)))\n","    print(data_folders)\n","    return data_folders\n","  \n","train_folders = maybe_extract(train_filename)\n","test_folders = maybe_extract(test_filename)\n","\n","image_size = 28  # Pixel width and height.\n","pixel_depth = 255.0  # Number of levels per pixel.\n","\n","def load_letter(folder, min_num_images):\n","    \"\"\"Load the data for a single letter label.\"\"\"\n","    image_files = os.listdir(folder)\n","    dataset = np.ndarray(shape=(len(image_files), image_size, image_size),\n","                         dtype=np.float32)\n","    print(folder)\n","    num_images = 0\n","    for image in image_files:\n","        image_file = os.path.join(folder, image)\n","        try:\n","            image_data = (ndimage.imread(image_file).astype(float) - \n","                        pixel_depth / 2) / pixel_depth\n","            if image_data.shape != (image_size, image_size):\n","                raise Exception('Unexpected image shape: %s' % str(image_data.shape))\n","            dataset[num_images, :, :] = image_data\n","            num_images = num_images + 1\n","        except IOError as e:\n","            print('Could not read:', image_file, ':', e, '- it\\'s ok, skipping.')\n","\n","    dataset = dataset[0:num_images, :, :]\n","    if num_images < min_num_images:\n","        raise Exception('Many fewer images than expected: %d < %d' %\n","                        (num_images, min_num_images))\n","\n","    print('Full dataset tensor:', dataset.shape)\n","    print('Mean:', np.mean(dataset))\n","    print('Standard deviation:', np.std(dataset))\n","    return dataset\n","        \n","def maybe_pickle(data_folders, min_num_images_per_class, force=False):\n","    dataset_names = []\n","    for folder in data_folders:\n","        set_filename = folder + '.pickle'\n","        dataset_names.append(set_filename)\n","        if os.path.exists(set_filename) and not force:\n","          # You may override by setting force=True.\n","          print('%s already present - Skipping pickling.' % set_filename)\n","        else:\n","            print('Pickling %s.' % set_filename)\n","            dataset = load_letter(folder, min_num_images_per_class)\n","            try:\n","                with open(set_filename, 'wb') as f:\n","                    pickle.dump(dataset, f, pickle.HIGHEST_PROTOCOL)\n","            except Exception as e:\n","                print('Unable to save data to', set_filename, ':', e)\n","\n","    return dataset_names\n","\n","train_datasets = maybe_pickle(train_folders, 45000)\n","test_datasets = maybe_pickle(test_folders, 1800)\n","\n","def make_arrays(nb_rows, img_size):\n","    if nb_rows:\n","        dataset = np.ndarray((nb_rows, img_size, img_size), dtype=np.float32)\n","        labels = np.ndarray(nb_rows, dtype=np.int32)\n","    else:\n","        dataset, labels = None, None\n","    return dataset, labels\n","\n","def merge_datasets(pickle_files, train_size, valid_size=0):\n","    num_classes = len(pickle_files)\n","    valid_dataset, valid_labels = make_arrays(valid_size, image_size)\n","    train_dataset, train_labels = make_arrays(train_size, image_size)\n","    vsize_per_class = valid_size // num_classes\n","    tsize_per_class = train_size // num_classes\n","\n","    start_v, start_t = 0, 0\n","    end_v, end_t = vsize_per_class, tsize_per_class\n","    end_l = vsize_per_class+tsize_per_class\n","    for label, pickle_file in enumerate(pickle_files):       \n","        try:\n","            with open(pickle_file, 'rb') as f:\n","                letter_set = pickle.load(f)\n","                # let's shuffle the letters to have random validation and training set\n","                np.random.shuffle(letter_set)\n","                if valid_dataset is not None:\n","                    valid_letter = letter_set[:vsize_per_class, :, :]\n","                    valid_dataset[start_v:end_v, :, :] = valid_letter\n","                    valid_labels[start_v:end_v] = label\n","                    start_v += vsize_per_class\n","                    end_v += vsize_per_class\n","\n","                train_letter = letter_set[vsize_per_class:end_l, :, :]\n","                train_dataset[start_t:end_t, :, :] = train_letter\n","                train_labels[start_t:end_t] = label\n","                start_t += tsize_per_class\n","                end_t += tsize_per_class\n","        except Exception as e:\n","            print('Unable to process data from', pickle_file, ':', e)\n","            raise\n","\n","    return valid_dataset, valid_labels, train_dataset, train_labels\n","\n","            \n","train_size = 200000\n","valid_size = 10000\n","test_size = 10000\n","\n","valid_dataset, valid_labels, train_dataset, train_labels = merge_datasets(\n","  train_datasets, train_size, valid_size)\n","_, _, test_dataset, test_labels = merge_datasets(test_datasets, test_size)\n","\n","print('Training:', train_dataset.shape, train_labels.shape)\n","print('Validation:', valid_dataset.shape, valid_labels.shape)\n","print('Testing:', test_dataset.shape, test_labels.shape)\n","\n","def randomize(dataset, labels):\n","    permutation = np.random.permutation(labels.shape[0])\n","    shuffled_dataset = dataset[permutation,:,:]\n","    shuffled_labels = labels[permutation]\n","    return shuffled_dataset, shuffled_labels\n","train_dataset, train_labels = randomize(train_dataset, train_labels)\n","test_dataset, test_labels = randomize(test_dataset, test_labels)\n","valid_dataset, valid_labels = randomize(valid_dataset, valid_labels)\n","\n","\n","pickle_file = os.path.join(data_root, 'notMNIST.pickle')\n","\n","try:\n","    f = open(pickle_file, 'wb')\n","    save = {\n","        'train_dataset': train_dataset,\n","        'train_labels': train_labels,\n","        'valid_dataset': valid_dataset,\n","        'valid_labels': valid_labels,\n","        'test_dataset': test_dataset,\n","        'test_labels': test_labels,\n","    }\n","    pickle.dump(save, f, pickle.HIGHEST_PROTOCOL)\n","    f.close()\n","except Exception as e:\n","    print('Unable to save data to', pickle_file, ':', e)\n","    raise"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: termcolor in /usr/local/lib/python3.6/dist-packages (1.1.0)\n","\u001b[32m@ pwd\u001b[0m\n","/content\n","\u001b[32m@ ls -al\u001b[0m\n","total 16\n","drwxr-xr-x 1 root root 4096 Nov 29 18:21 .\n","drwxr-xr-x 1 root root 4096 Dec  3 22:06 ..\n","drwxr-xr-x 1 root root 4096 Nov 29 18:21 .config\n","drwxr-xr-x 2 root root 4096 Nov 29 18:21 sample_data\n","\u001b[32m@ mkdir -p data\u001b[0m\n","\u001b[32m@ ls -al\u001b[0m\n","total 20\n","drwxr-xr-x 1 root root 4096 Dec  3 22:11 .\n","drwxr-xr-x 1 root root 4096 Dec  3 22:06 ..\n","drwxr-xr-x 1 root root 4096 Nov 29 18:21 .config\n","drwxr-xr-x 2 root root 4096 Dec  3 22:11 data\n","drwxr-xr-x 2 root root 4096 Nov 29 18:21 sample_data\n","Attempting to download: notMNIST_large.tar.gz\n","0%....5%....10%....15%....20%....25%....30%....35%....40%....45%....50%....55%....60%....65%....70%....75%....80%....85%....90%....95%....100%\n","Download Complete!\n","Found and verified ./data/notMNIST_large.tar.gz\n","Attempting to download: notMNIST_small.tar.gz\n","0%....5%....10%....15%....20%....25%....30%....35%....40%....45%....50%....55%....60%....65%....70%....75%....80%....85%....90%....95%....100%\n","Download Complete!\n","Found and verified ./data/notMNIST_small.tar.gz\n","Extracting data for ./data/notMNIST_large. This may take a while. Please wait.\n","['./data/notMNIST_large/A', './data/notMNIST_large/B', './data/notMNIST_large/C', './data/notMNIST_large/D', './data/notMNIST_large/E', './data/notMNIST_large/F', './data/notMNIST_large/G', './data/notMNIST_large/H', './data/notMNIST_large/I', './data/notMNIST_large/J']\n","Extracting data for ./data/notMNIST_small. This may take a while. Please wait.\n","['./data/notMNIST_small/A', './data/notMNIST_small/B', './data/notMNIST_small/C', './data/notMNIST_small/D', './data/notMNIST_small/E', './data/notMNIST_small/F', './data/notMNIST_small/G', './data/notMNIST_small/H', './data/notMNIST_small/I', './data/notMNIST_small/J']\n","Pickling ./data/notMNIST_large/A.pickle.\n","./data/notMNIST_large/A\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:116: DeprecationWarning: `imread` is deprecated!\n","`imread` is deprecated in SciPy 1.0.0.\n","Use ``matplotlib.pyplot.imread`` instead.\n"],"name":"stderr"},{"output_type":"stream","text":["Could not read: ./data/notMNIST_large/A/Um9tYW5hIEJvbGQucGZi.png : cannot identify image file './data/notMNIST_large/A/Um9tYW5hIEJvbGQucGZi.png' - it's ok, skipping.\n","Could not read: ./data/notMNIST_large/A/SG90IE11c3RhcmQgQlROIFBvc3Rlci50dGY=.png : cannot identify image file './data/notMNIST_large/A/SG90IE11c3RhcmQgQlROIFBvc3Rlci50dGY=.png' - it's ok, skipping.\n","Could not read: ./data/notMNIST_large/A/RnJlaWdodERpc3BCb29rSXRhbGljLnR0Zg==.png : cannot identify image file './data/notMNIST_large/A/RnJlaWdodERpc3BCb29rSXRhbGljLnR0Zg==.png' - it's ok, skipping.\n","Full dataset tensor: (52909, 28, 28)\n","Mean: -0.12825006\n","Standard deviation: 0.44312054\n","Pickling ./data/notMNIST_large/B.pickle.\n","./data/notMNIST_large/B\n","Could not read: ./data/notMNIST_large/B/TmlraXNFRi1TZW1pQm9sZEl0YWxpYy5vdGY=.png : cannot identify image file './data/notMNIST_large/B/TmlraXNFRi1TZW1pQm9sZEl0YWxpYy5vdGY=.png' - it's ok, skipping.\n","Full dataset tensor: (52911, 28, 28)\n","Mean: -0.0075630425\n","Standard deviation: 0.45449144\n","Pickling ./data/notMNIST_large/C.pickle.\n","./data/notMNIST_large/C\n","Full dataset tensor: (52912, 28, 28)\n","Mean: -0.1422581\n","Standard deviation: 0.43980613\n","Pickling ./data/notMNIST_large/D.pickle.\n","./data/notMNIST_large/D\n","Could not read: ./data/notMNIST_large/D/VHJhbnNpdCBCb2xkLnR0Zg==.png : cannot identify image file './data/notMNIST_large/D/VHJhbnNpdCBCb2xkLnR0Zg==.png' - it's ok, skipping.\n","Full dataset tensor: (52911, 28, 28)\n","Mean: -0.057367716\n","Standard deviation: 0.45564747\n","Pickling ./data/notMNIST_large/E.pickle.\n","./data/notMNIST_large/E\n","Full dataset tensor: (52912, 28, 28)\n","Mean: -0.06989912\n","Standard deviation: 0.45294175\n","Pickling ./data/notMNIST_large/F.pickle.\n","./data/notMNIST_large/F\n","Full dataset tensor: (52912, 28, 28)\n","Mean: -0.12558314\n","Standard deviation: 0.44708964\n","Pickling ./data/notMNIST_large/G.pickle.\n","./data/notMNIST_large/G\n","Full dataset tensor: (52912, 28, 28)\n","Mean: -0.094581604\n","Standard deviation: 0.44623974\n","Pickling ./data/notMNIST_large/H.pickle.\n","./data/notMNIST_large/H\n","Full dataset tensor: (52912, 28, 28)\n","Mean: -0.06852206\n","Standard deviation: 0.45423168\n","Pickling ./data/notMNIST_large/I.pickle.\n","./data/notMNIST_large/I\n","Full dataset tensor: (52912, 28, 28)\n","Mean: 0.030786201\n","Standard deviation: 0.46889874\n","Pickling ./data/notMNIST_large/J.pickle.\n","./data/notMNIST_large/J\n","Full dataset tensor: (52911, 28, 28)\n","Mean: -0.15335809\n","Standard deviation: 0.443656\n","Pickling ./data/notMNIST_small/A.pickle.\n","./data/notMNIST_small/A\n","Could not read: ./data/notMNIST_small/A/RGVtb2NyYXRpY2FCb2xkT2xkc3R5bGUgQm9sZC50dGY=.png : cannot identify image file './data/notMNIST_small/A/RGVtb2NyYXRpY2FCb2xkT2xkc3R5bGUgQm9sZC50dGY=.png' - it's ok, skipping.\n","Full dataset tensor: (1872, 28, 28)\n","Mean: -0.13262635\n","Standard deviation: 0.4451279\n","Pickling ./data/notMNIST_small/B.pickle.\n","./data/notMNIST_small/B\n","Full dataset tensor: (1873, 28, 28)\n","Mean: 0.005356083\n","Standard deviation: 0.45711535\n","Pickling ./data/notMNIST_small/C.pickle.\n","./data/notMNIST_small/C\n","Full dataset tensor: (1873, 28, 28)\n","Mean: -0.14152056\n","Standard deviation: 0.44269025\n","Pickling ./data/notMNIST_small/D.pickle.\n","./data/notMNIST_small/D\n","Full dataset tensor: (1873, 28, 28)\n","Mean: -0.049216677\n","Standard deviation: 0.4597589\n","Pickling ./data/notMNIST_small/E.pickle.\n","./data/notMNIST_small/E\n","Full dataset tensor: (1873, 28, 28)\n","Mean: -0.059914753\n","Standard deviation: 0.45734954\n","Pickling ./data/notMNIST_small/F.pickle.\n","./data/notMNIST_small/F\n","Could not read: ./data/notMNIST_small/F/Q3Jvc3NvdmVyIEJvbGRPYmxpcXVlLnR0Zg==.png : cannot identify image file './data/notMNIST_small/F/Q3Jvc3NvdmVyIEJvbGRPYmxpcXVlLnR0Zg==.png' - it's ok, skipping.\n","Full dataset tensor: (1872, 28, 28)\n","Mean: -0.11818536\n","Standard deviation: 0.45227864\n","Pickling ./data/notMNIST_small/G.pickle.\n","./data/notMNIST_small/G\n","Full dataset tensor: (1872, 28, 28)\n","Mean: -0.092550345\n","Standard deviation: 0.44900593\n","Pickling ./data/notMNIST_small/H.pickle.\n","./data/notMNIST_small/H\n","Full dataset tensor: (1872, 28, 28)\n","Mean: -0.058689263\n","Standard deviation: 0.45875895\n","Pickling ./data/notMNIST_small/I.pickle.\n","./data/notMNIST_small/I\n","Full dataset tensor: (1872, 28, 28)\n","Mean: 0.05264508\n","Standard deviation: 0.47189343\n","Pickling ./data/notMNIST_small/J.pickle.\n","./data/notMNIST_small/J\n","Full dataset tensor: (1872, 28, 28)\n","Mean: -0.15168923\n","Standard deviation: 0.44801354\n","Training: (200000, 28, 28) (200000,)\n","Validation: (10000, 28, 28) (10000,)\n","Testing: (10000, 28, 28) (10000,)\n"],"name":"stdout"}]},{"metadata":{"id":"9vWlHOHe_6UK","colab_type":"text"},"cell_type":"markdown","source":["## Load datasets\n","\n","First reload the data we generated in `Assignment2-1_Data_Curation.ipynb`."]},{"metadata":{"cellView":"both","colab_type":"code","id":"JLpLa8Jt7Vu4","colab":{}},"cell_type":"code","source":["# These are all the modules we'll be using later. Make sure you can import them\n","# before proceeding further.\n","from __future__ import print_function\n","import numpy as np\n","import tensorflow as tf\n","from six.moves import cPickle as pickle\n","from six.moves import range\n","import os\n","\n","#configuration for gpu usage\n","conf = tf.ConfigProto()\n","conf.gpu_options.per_process_gpu_memory_fraction = 0.4\n","conf.gpu_options.allow_growth = True\n","os.environ['CUDA_VISIBLE_DEVICES']='0'"],"execution_count":0,"outputs":[]},{"metadata":{"cellView":"both","colab_type":"code","executionInfo":{"status":"ok","timestamp":1543875406514,"user_tz":-540,"elapsed":346241,"user":{"displayName":"Seol JaeWan","photoUrl":"","userId":"07332071038155379708"}},"id":"y3-cj1bpmuxc","outputId":"cddea7f1-d737-4978-d5f5-42082b446b97","colab":{"base_uri":"https://localhost:8080/","height":73}},"cell_type":"code","source":["pickle_file = 'data/notMNIST.pickle'\n","\n","with open(pickle_file, 'rb') as f:\n","    save = pickle.load(f)\n","    train_dataset = save['train_dataset']\n","    train_labels = save['train_labels']\n","    valid_dataset = save['valid_dataset']\n","    valid_labels = save['valid_labels']\n","    test_dataset = save['test_dataset']\n","    test_labels = save['test_labels']\n","    del save  # hint to help gc free up memory\n","    print('Training set', train_dataset.shape, train_labels.shape)\n","    print('Validation set', valid_dataset.shape, valid_labels.shape)\n","    print('Test set', test_dataset.shape, test_labels.shape)"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Training set (200000, 28, 28) (200000,)\n","Validation set (10000, 28, 28) (10000,)\n","Test set (10000, 28, 28) (10000,)\n"],"name":"stdout"}]},{"metadata":{"colab_type":"text","id":"L7aHrm6nGDMB"},"cell_type":"markdown","source":["Reformat into a shape that's more adapted to the models we're going to train:\n","- data as a flat matrix,\n","- labels as float 1-hot encodings."]},{"metadata":{"cellView":"both","colab_type":"code","executionInfo":{"status":"ok","timestamp":1543875406881,"user_tz":-540,"elapsed":346590,"user":{"displayName":"Seol JaeWan","photoUrl":"","userId":"07332071038155379708"}},"id":"IRSyYiIIGIzS","outputId":"7a64d929-52f1-472c-ec8e-2777f88ac03e","colab":{"base_uri":"https://localhost:8080/","height":73}},"cell_type":"code","source":["image_size = 28\n","num_labels = 10\n","\n","def reformat(dataset, labels):\n","    dataset = dataset.reshape((-1, image_size * image_size)).astype(np.float32)\n","    # Map 0 to [1.0, 0.0, 0.0 ...], 1 to [0.0, 1.0, 0.0 ...]\n","    labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n","    return dataset, labels\n","train_dataset, train_labels = reformat(train_dataset, train_labels)\n","valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n","test_dataset, test_labels = reformat(test_dataset, test_labels)\n","print('Training set', train_dataset.shape, train_labels.shape)\n","print('Validation set', valid_dataset.shape, valid_labels.shape)\n","print('Test set', test_dataset.shape, test_labels.shape)"],"execution_count":5,"outputs":[{"output_type":"stream","text":["Training set (200000, 784) (200000, 10)\n","Validation set (10000, 784) (10000, 10)\n","Test set (10000, 784) (10000, 10)\n"],"name":"stdout"}]},{"metadata":{"colab_type":"text","id":"nCLVqyQ5vPPH"},"cell_type":"markdown","source":["## TensorFlow tutorial: Fully Connected Network\n","\n","We're first going to train a **fully connected network** with *1 hidden layer* with *1024 units* using stochastic gradient descent (SGD).\n","\n","TensorFlow works like this:\n","* First you describe the computation that you want to see performed: what the inputs, the variables, and the operations look like. These get created as nodes over a computation graph. This description is all contained within the block below:\n","\n","      with graph.as_default():\n","          ...\n","\n","* Then you can run the operations on this graph as many times as you want by calling `session.run()`, providing it outputs to fetch from the graph that get returned. This runtime operation is all contained in the block below:\n","\n","      with tf.Session(graph=graph) as session:\n","          ...\n","\n","Let's load all the data into TensorFlow and build the computation graph corresponding to our training:"]},{"metadata":{"id":"XgC60TMV_6UX","colab_type":"code","colab":{}},"cell_type":"code","source":["batch_size = 128\n","nn_hidden = 1024\n","\n","graph = tf.Graph()\n","with graph.as_default():\n","    # Input data. For the training data, we use a placeholder that will be fed\n","    # at run time with a training minibatch.\n","    tf_dataset = tf.placeholder(tf.float32,\n","                                      shape=(None, image_size * image_size))\n","    tf_labels = tf.placeholder(tf.float32, shape=(None, num_labels))\n","    \n","    # Variables. \n","    w1 = tf.Variable(tf.truncated_normal([image_size * image_size, nn_hidden]))\n","    b1 = tf.Variable(tf.zeros([nn_hidden]))\n","    w2 = tf.Variable(tf.truncated_normal([nn_hidden, num_labels]))\n","    b2 = tf.Variable(tf.zeros([num_labels]))\n","    \n","    # Training computation.\n","    hidden = tf.tanh(tf.matmul(tf_dataset, w1) + b1)\n","    logits = tf.matmul(hidden, w2) + b2\n","    \n","    loss = tf.reduce_mean(\n","        tf.nn.softmax_cross_entropy_with_logits_v2(labels=tf_labels, logits=logits))\n","    \n","    # Optimizer.\n","    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n","  \n","    # Predictions for the training, validation, and test data.\n","    prediction = tf.nn.softmax(logits)"],"execution_count":0,"outputs":[]},{"metadata":{"colab_type":"text","id":"KQcL4uqISHjP"},"cell_type":"markdown","source":["Let's run this computation and iterate:"]},{"metadata":{"id":"jY1AlMC5_6Ub","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":614},"outputId":"06a13734-bbbb-498a-d41a-b25334c0b3de","executionInfo":{"status":"ok","timestamp":1543875435902,"user_tz":-540,"elapsed":375571,"user":{"displayName":"Seol JaeWan","photoUrl":"","userId":"07332071038155379708"}}},"cell_type":"code","source":["num_steps = 10000\n","\n","def accuracy(predictions, labels):\n","    return (100.0 * np.sum(np.equal(np.argmax(predictions, 1), np.argmax(labels, 1)))\n","          / predictions.shape[0])\n","\n","with tf.Session(graph=graph) as session:\n","    tf.global_variables_initializer().run()\n","    print(\"Initialized\")\n","    for step in range(num_steps):\n","        # Pick an offset within the training data, which has been randomized.\n","        # Note: we could use better randomization across epochs.\n","        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n","        # Generate a minibatch.\n","        batch_data = train_dataset[offset:(offset + batch_size), :]\n","        batch_labels = train_labels[offset:(offset + batch_size), :]\n","        # Prepare a dictionary telling the session where to feed the minibatch.\n","        # The key of the dictionary is the placeholder node of the graph to be fed,\n","        # and the value is the numpy array to feed to it.\n","        feed_dict_train={tf_dataset: batch_data, tf_labels: batch_labels}\n","        _, l, predictions = session.run([optimizer, loss, prediction], feed_dict=feed_dict_train)\n","        if (step % 1000 == 0):\n","            print(\"Minibatch loss at step %d: %f\" % (step, l))\n","            print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n","            valid_prediction = session.run(logits, feed_dict={tf_dataset: valid_dataset})\n","            print(\"Validation accuracy: %.1f%%\" % accuracy(valid_prediction, valid_labels))\n","                  \n","    test_prediction = session.run(prediction, feed_dict={tf_dataset: test_dataset})\n","    print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction, test_labels))\n","    saver = tf.train.Saver()\n","    saver.save(session, \"./model_checkpoints/my_model_final\")"],"execution_count":7,"outputs":[{"output_type":"stream","text":["Initialized\n","Minibatch loss at step 0: 34.344711\n","Minibatch accuracy: 12.5%\n","Validation accuracy: 27.0%\n","Minibatch loss at step 1000: 3.208072\n","Minibatch accuracy: 77.3%\n","Validation accuracy: 78.9%\n","Minibatch loss at step 2000: 1.745303\n","Minibatch accuracy: 82.8%\n","Validation accuracy: 80.7%\n","Minibatch loss at step 3000: 1.150633\n","Minibatch accuracy: 78.9%\n","Validation accuracy: 81.8%\n","Minibatch loss at step 4000: 0.866558\n","Minibatch accuracy: 86.7%\n","Validation accuracy: 82.3%\n","Minibatch loss at step 5000: 1.392125\n","Minibatch accuracy: 81.2%\n","Validation accuracy: 78.6%\n","Minibatch loss at step 6000: 0.749648\n","Minibatch accuracy: 87.5%\n","Validation accuracy: 81.6%\n","Minibatch loss at step 7000: 0.611268\n","Minibatch accuracy: 88.3%\n","Validation accuracy: 81.6%\n","Minibatch loss at step 8000: 0.762344\n","Minibatch accuracy: 86.7%\n","Validation accuracy: 79.4%\n","Minibatch loss at step 9000: 0.984730\n","Minibatch accuracy: 85.2%\n","Validation accuracy: 81.4%\n","Test accuracy: 89.1%\n"],"name":"stdout"}]},{"metadata":{"id":"2Ftje0Om_6Uf","colab_type":"text"},"cell_type":"markdown","source":["So far, you have built the model in a naive way. However, TensorFlow provides a module named tf.layers for your convenience. \n","\n","From now on, build the same model as above using layers module."]},{"metadata":{"id":"MyI7Vd20_6Ug","colab_type":"code","colab":{}},"cell_type":"code","source":["graph_l=tf.Graph()\n","with graph_l.as_default():\n","    tf_dataset_l=tf.placeholder(tf.float32, shape=(None, image_size * image_size))\n","    tf_labels_l=tf.placeholder(tf.float32, shape=(None, num_labels))\n","    \n","    dense = tf.layers.dense(tf_dataset_l, nn_hidden, activation=tf.tanh)\n","    logits_l = tf.layers.dense(dense, num_labels)\n","    \n","    #Loss\n","    loss_l = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=tf_labels_l, logits=logits_l))\n","    \n","    #Optimizer\n","    optimizer_l = tf.train.GradientDescentOptimizer(0.5).minimize(loss_l)\n","    \n","    #Predictions for the training\n","    prediction_l = tf.nn.softmax(logits_l)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"qziyp14x_6Ui","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":428},"outputId":"d0b73253-2536-4ca5-a214-a8ce69aa4a02","executionInfo":{"status":"ok","timestamp":1543875462711,"user_tz":-540,"elapsed":402353,"user":{"displayName":"Seol JaeWan","photoUrl":"","userId":"07332071038155379708"}}},"cell_type":"code","source":["with tf.Session(graph=graph_l, config=conf) as session_l:\n","    tf.global_variables_initializer().run()\n","    print(\"Initialized\")\n","    for step in range(num_steps):\n","        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n","        batch_data = train_dataset[offset:(offset + batch_size), :]\n","        batch_labels = train_labels[offset:(offset + batch_size), :].astype(float)\n","        feed_dict_l = {tf_dataset_l: batch_data, tf_labels_l: batch_labels}\n","        _, l_l, predictions_l = session_l.run([optimizer_l, loss_l, prediction_l], feed_dict=feed_dict_l)\n","        if(step % 1000 == 0):\n","            print(\"Minibatch loss at step %d: %f\" % (step, l_l))\n","            feed_dict_val_l = {tf_dataset_l: valid_dataset}\n","            valid_prediction_l = session_l.run(prediction_l, feed_dict={tf_dataset_l: valid_dataset, tf_labels_l: valid_labels})\n","            print(\"Validation accuracy: %.1f%%\" % accuracy(valid_prediction_l, valid_labels))\n","\n","    feed_dict_test_l = {tf_dataset_l: test_dataset}\n","    test_prediction_l = session_l.run(prediction_l, feed_dict=feed_dict_test_l)\n","    print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction_l, test_labels))\n","    saver = tf.train.Saver()\n","    saver.save(session_l, \"./model_checkpoints/my_model_final_using_layers\")"],"execution_count":9,"outputs":[{"output_type":"stream","text":["Initialized\n","Minibatch loss at step 0: 2.388520\n","Validation accuracy: 44.1%\n","Minibatch loss at step 1000: 0.584907\n","Validation accuracy: 83.7%\n","Minibatch loss at step 2000: 0.471201\n","Validation accuracy: 85.3%\n","Minibatch loss at step 3000: 0.424750\n","Validation accuracy: 85.7%\n","Minibatch loss at step 4000: 0.354131\n","Validation accuracy: 86.4%\n","Minibatch loss at step 5000: 0.482221\n","Validation accuracy: 86.7%\n","Minibatch loss at step 6000: 0.330496\n","Validation accuracy: 87.6%\n","Minibatch loss at step 7000: 0.418526\n","Validation accuracy: 87.3%\n","Minibatch loss at step 8000: 0.255093\n","Validation accuracy: 87.6%\n","Minibatch loss at step 9000: 0.427875\n","Validation accuracy: 88.0%\n","Test accuracy: 93.3%\n"],"name":"stdout"}]},{"metadata":{"id":"f_dAd8vI_6Um","colab_type":"text"},"cell_type":"markdown","source":["---\n","Problem 1\n","-------\n","\n","**Describe below** why there is a difference in an accuracy between the graph using layer module and the graph which is built in a naive way.\n","\n","\n","\n","\n","\n","---"]},{"metadata":{"id":"fzngWzVVUCfj","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":36},"outputId":"3e9d3411-dbea-4aa9-d8b6-99ff6c604074","executionInfo":{"status":"ok","timestamp":1543875462712,"user_tz":-540,"elapsed":402341,"user":{"displayName":"Seol JaeWan","photoUrl":"","userId":"07332071038155379708"}}},"cell_type":"code","source":["print('temp')"],"execution_count":10,"outputs":[{"output_type":"stream","text":["temp\n"],"name":"stdout"}]},{"metadata":{"id":"lV1cURnK_6Um","colab_type":"text"},"cell_type":"markdown","source":["Describe here"]},{"metadata":{"colab_type":"text","id":"7omWxtvLLxik"},"cell_type":"markdown","source":["---\n","Problem 2\n","-------\n","\n","Try to get the best performance you can using a multi-layer model! (It doesn't matter whether you implement it in a naive way or using layer module. HOWEVER, you CANNOT use other type of layers such as conv.) \n","\n","The best reported test accuracy using a deep network is [97.1%](http://yaroslavvb.blogspot.kr/2011/09/notmnist-dataset.html?showComment=1391023266211#c8758720086795711595). You may use techniques below.\n","\n","1. Experiment with different hyperparameters: num_steps, learning rate, etc.\n","2. We used a fixed learning rate $\\epsilon$ for gradient descent. Implement an annealing schedule for the gradient descent learning rate ([more info](http://cs231n.github.io/neural-networks-3/#anneal)). *Hint*. Try using `tf.train.exponential_decay`.    \n","3. We used a $\\tanh$ activation function for our hidden layer. Experiment with other activation functions included in TensorFlow.\n","4. Extend the network to multiple hidden layers. Experiment with the layer sizes. Adding another hidden layer means you will need to adjust the code. \n","5. Introduce and tune regularization method (e.g. L2 regularization) for your model. Remeber that L2 amounts to adding a penalty on the norm of the weights to the loss. In TensorFlow, you can compute the L2 loss for a tensor `t` using `nn.l2_loss(t)`. The right amount of regularization should imporve your validation / test accuracy.\n","6. Introduce Dropout on the hidden layer of the neural network. Remember: Dropout should only be introduced during training, not evaluation, otherwise your evaluation results would be stochastic as well. TensorFlow provides nn.dropout() for that, but you have to make sure it's only inserted during training.\n","\n","**Evaluation:** You will get full credit if your best test accuracy exceeds 93%. Save your best perfoming model as my_model_final using saver. (Refer to the cell above) \n","\n","---"]},{"metadata":{"id":"JJhgdLU2_6Uo","colab_type":"code","colab":{}},"cell_type":"code","source":["num_steps = 15000\n","nn_hdim = 1024\n","learning_rate=0.01\n","batch_size = 128\n","reg_lambda = 0.005\n","\n","my_graph = tf.Graph()\n","with my_graph.as_default():\n","    input_data = tf.placeholder(tf.float32, shape=(None, image_size * image_size))\n","    input_labels = tf.placeholder(tf.float32, shape=(None, num_labels))\n","    is_training = tf.placeholder(tf.bool)\n","    \n","    \n","    # L2 regularizer\n","    l2_regularizer = tf.contrib.layers.l2_regularizer(scale=reg_lambda)\n","    hidden_layer = tf.layers.dense(inputs=input_data, units=nn_hdim, activation=tf.nn.elu, kernel_regularizer=l2_regularizer)\n","    hidden_layer = tf.layers.dropout(hidden_layer, training=is_training)\n","    output_layer = tf.layers.dense(inputs=hidden_layer, units=num_labels, kernel_regularizer=l2_regularizer)\n","    \n","    # logits\n","    logits = output_layer\n","    \n","    # loss\n","    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=input_labels, logits=logits))\n","    \n","    # optimizer\n","    optimizer = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n","    \n","    # prediction\n","    prediction = tf.nn.softmax(logits)\n","    \n","    \n","def random_sample(data, labels, batch_size):\n","    random_index = np.random.permutation(len(data))[:batch_size]\n","    batch_data = data[random_index]\n","    batch_labels = labels[random_index]\n","    return batch_data, batch_labels\n","    "],"execution_count":0,"outputs":[]},{"metadata":{"id":"n7s4x0hDX523","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1361},"outputId":"fdbfd8e7-9298-484f-a7d3-20df4a401480","executionInfo":{"status":"ok","timestamp":1543886874041,"user_tz":-540,"elapsed":476,"user":{"displayName":"Seol JaeWan","photoUrl":"","userId":"07332071038155379708"}}},"cell_type":"code","source":["from itertools import product\n","\n","nn_hdims = [1024, 2048]\n","learning_rates = [0.001, 0.01, 0.05]\n","batch_sizes = [128, 256, 512]\n","reg_lambdas = [0.001, 0.005, 0.01, 0.05]\n","\n","result = {} # key: case, value: test accuracy\n","cases = list(product(nn_hdims, learning_rates, batch_sizes, reg_lambdas))\n","\n","for j, case in enumerate(cases):\n","    nn_hdim, learning_rate, batch_size, reg_lambda = case\n","\n","\n","    with tf.Session(graph=my_graph, config=conf) as my_session:\n","        tf.global_variables_initializer().run()\n","\n","        for i in range(num_steps):\n","            batch_data, batch_labels = random_sample(train_dataset, train_labels, batch_size)\n","            feed_dict = {\n","                input_data: batch_data,\n","                input_labels: batch_labels,\n","                is_training: True\n","            }\n","            train_loss, _, predictions = my_session.run([loss, optimizer, prediction], feed_dict=feed_dict)\n","\n","\n","            # print train performance(include validation performance) per 1000 steps\n","#             if i % 1000 == 0:\n","#                 train_accuracy = accuracy(predictions, batch_labels)\n","#                 print('step: %4d, train loss: %.4f, train accuracy: %.2f%%' % (i, train_loss, train_accuracy))\n","#                 valid_feed_dict = {\n","#                     input_data: valid_dataset,\n","#                     input_labels: valid_labels,\n","#                     is_training: False\n","#                 }\n","#                 valid_loss, valid_predictions = my_session.run([loss, prediction], feed_dict=valid_feed_dict)\n","#                 valid_accuracy = accuracy(valid_predictions, valid_labels)\n","#                 print('step: %4d, valid loss: %.4f, valid accuracy: %2.f%%' % (i, valid_loss, valid_accuracy))\n","\n","\n","        # test accuracy\n","        test_feed_dict = {\n","            input_data: test_dataset,\n","            input_labels: test_labels,\n","            is_training: False\n","        }\n","        test_predictions = my_session.run(prediction, feed_dict=test_feed_dict)\n","        test_accuracy = accuracy(test_predictions, test_labels)\n","        print(('case: ' + str(case)).ljust(32, ' '), 'total case: %d / %d, test accuracy: %.2f%%' % (j, len(cases), test_accuracy))\n","        result[case] = test_accuracy\n","     "],"execution_count":14,"outputs":[{"output_type":"stream","text":["case: (1024, 0.001, 128, 0.001)  total case: 0 / 72, test accuracy: 91.00%\n","case: (1024, 0.001, 128, 0.005)  total case: 1 / 72, test accuracy: 90.81%\n","case: (1024, 0.001, 128, 0.01)   total case: 2 / 72, test accuracy: 91.24%\n","case: (1024, 0.001, 128, 0.05)   total case: 3 / 72, test accuracy: 90.75%\n","case: (1024, 0.001, 256, 0.001)  total case: 4 / 72, test accuracy: 91.33%\n","case: (1024, 0.001, 256, 0.005)  total case: 5 / 72, test accuracy: 91.24%\n","case: (1024, 0.001, 256, 0.01)   total case: 6 / 72, test accuracy: 91.52%\n","case: (1024, 0.001, 256, 0.05)   total case: 7 / 72, test accuracy: 91.66%\n","case: (1024, 0.001, 512, 0.001)  total case: 8 / 72, test accuracy: 93.02%\n","case: (1024, 0.001, 512, 0.005)  total case: 9 / 72, test accuracy: 92.59%\n","case: (1024, 0.001, 512, 0.01)   total case: 10 / 72, test accuracy: 93.69%\n","case: (1024, 0.001, 512, 0.05)   total case: 11 / 72, test accuracy: 92.86%\n","case: (1024, 0.01, 128, 0.001)   total case: 12 / 72, test accuracy: 90.90%\n","case: (1024, 0.01, 128, 0.005)   total case: 13 / 72, test accuracy: 90.03%\n","case: (1024, 0.01, 128, 0.01)    total case: 14 / 72, test accuracy: 90.58%\n","case: (1024, 0.01, 128, 0.05)    total case: 15 / 72, test accuracy: 89.04%\n","case: (1024, 0.01, 256, 0.001)   total case: 16 / 72, test accuracy: 92.41%\n","case: (1024, 0.01, 256, 0.005)   total case: 17 / 72, test accuracy: 92.26%\n","case: (1024, 0.01, 256, 0.01)    total case: 18 / 72, test accuracy: 92.58%\n","case: (1024, 0.01, 256, 0.05)    total case: 19 / 72, test accuracy: 91.44%\n","case: (1024, 0.01, 512, 0.001)   total case: 20 / 72, test accuracy: 93.49%\n","case: (1024, 0.01, 512, 0.005)   total case: 21 / 72, test accuracy: 93.42%\n","case: (1024, 0.01, 512, 0.01)    total case: 22 / 72, test accuracy: 93.63%\n","case: (1024, 0.01, 512, 0.05)    total case: 23 / 72, test accuracy: 93.30%\n","case: (1024, 0.05, 128, 0.001)   total case: 24 / 72, test accuracy: 90.92%\n","case: (1024, 0.05, 128, 0.005)   total case: 25 / 72, test accuracy: 90.01%\n","case: (1024, 0.05, 128, 0.01)    total case: 26 / 72, test accuracy: 89.54%\n","case: (1024, 0.05, 128, 0.05)    total case: 27 / 72, test accuracy: 89.73%\n","case: (1024, 0.05, 256, 0.001)   total case: 28 / 72, test accuracy: 92.06%\n","case: (1024, 0.05, 256, 0.005)   total case: 29 / 72, test accuracy: 92.36%\n","case: (1024, 0.05, 256, 0.01)    total case: 30 / 72, test accuracy: 92.02%\n","case: (1024, 0.05, 256, 0.05)    total case: 31 / 72, test accuracy: 92.15%\n","case: (1024, 0.05, 512, 0.001)   total case: 32 / 72, test accuracy: 93.21%\n","case: (1024, 0.05, 512, 0.005)   total case: 33 / 72, test accuracy: 93.02%\n","case: (1024, 0.05, 512, 0.01)    total case: 34 / 72, test accuracy: 93.57%\n","case: (1024, 0.05, 512, 0.05)    total case: 35 / 72, test accuracy: 93.40%\n","case: (2048, 0.001, 128, 0.001)  total case: 36 / 72, test accuracy: 90.21%\n","case: (2048, 0.001, 128, 0.005)  total case: 37 / 72, test accuracy: 90.50%\n","case: (2048, 0.001, 128, 0.01)   total case: 38 / 72, test accuracy: 90.31%\n","case: (2048, 0.001, 128, 0.05)   total case: 39 / 72, test accuracy: 90.73%\n","case: (2048, 0.001, 256, 0.001)  total case: 40 / 72, test accuracy: 92.02%\n","case: (2048, 0.001, 256, 0.005)  total case: 41 / 72, test accuracy: 90.11%\n","case: (2048, 0.001, 256, 0.01)   total case: 42 / 72, test accuracy: 92.15%\n","case: (2048, 0.001, 256, 0.05)   total case: 43 / 72, test accuracy: 91.88%\n","case: (2048, 0.001, 512, 0.001)  total case: 44 / 72, test accuracy: 93.40%\n","case: (2048, 0.001, 512, 0.005)  total case: 45 / 72, test accuracy: 93.11%\n","case: (2048, 0.001, 512, 0.01)   total case: 46 / 72, test accuracy: 93.57%\n","case: (2048, 0.001, 512, 0.05)   total case: 47 / 72, test accuracy: 93.70%\n","case: (2048, 0.01, 128, 0.001)   total case: 48 / 72, test accuracy: 90.63%\n","case: (2048, 0.01, 128, 0.005)   total case: 49 / 72, test accuracy: 90.69%\n","case: (2048, 0.01, 128, 0.01)    total case: 50 / 72, test accuracy: 88.07%\n","case: (2048, 0.01, 128, 0.05)    total case: 51 / 72, test accuracy: 90.18%\n","case: (2048, 0.01, 256, 0.001)   total case: 52 / 72, test accuracy: 92.42%\n","case: (2048, 0.01, 256, 0.005)   total case: 53 / 72, test accuracy: 92.39%\n","case: (2048, 0.01, 256, 0.01)    total case: 54 / 72, test accuracy: 91.77%\n","case: (2048, 0.01, 256, 0.05)    total case: 55 / 72, test accuracy: 92.00%\n","case: (2048, 0.01, 512, 0.001)   total case: 56 / 72, test accuracy: 93.75%\n","case: (2048, 0.01, 512, 0.005)   total case: 57 / 72, test accuracy: 93.55%\n","case: (2048, 0.01, 512, 0.01)    total case: 58 / 72, test accuracy: 93.24%\n","case: (2048, 0.01, 512, 0.05)    total case: 59 / 72, test accuracy: 93.08%\n","case: (2048, 0.05, 128, 0.001)   total case: 60 / 72, test accuracy: 90.12%\n","case: (2048, 0.05, 128, 0.005)   total case: 61 / 72, test accuracy: 89.88%\n","case: (2048, 0.05, 128, 0.01)    total case: 62 / 72, test accuracy: 89.76%\n","case: (2048, 0.05, 128, 0.05)    total case: 63 / 72, test accuracy: 89.17%\n","case: (2048, 0.05, 256, 0.001)   total case: 64 / 72, test accuracy: 92.32%\n","case: (2048, 0.05, 256, 0.005)   total case: 65 / 72, test accuracy: 91.61%\n","case: (2048, 0.05, 256, 0.01)    total case: 66 / 72, test accuracy: 92.00%\n","case: (2048, 0.05, 256, 0.05)    total case: 67 / 72, test accuracy: 92.15%\n","case: (2048, 0.05, 512, 0.001)   total case: 68 / 72, test accuracy: 93.44%\n","case: (2048, 0.05, 512, 0.005)   total case: 69 / 72, test accuracy: 93.02%\n","case: (2048, 0.05, 512, 0.01)    total case: 70 / 72, test accuracy: 92.50%\n","case: (2048, 0.05, 512, 0.05)    total case: 71 / 72, test accuracy: 93.27%\n"],"name":"stdout"}]},{"metadata":{"id":"9gbPG-NDi4Jl","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":36},"outputId":"0224283b-7f5c-4082-f600-b6e15d064e44","executionInfo":{"status":"ok","timestamp":1543887556756,"user_tz":-540,"elapsed":1292,"user":{"displayName":"Seol JaeWan","photoUrl":"","userId":"07332071038155379708"}}},"cell_type":"code","source":["temp = []\n","for key in result :\n","    temp.append((key, result[key]))\n","    \n","temp.sort(reverse=True, key=(lambda x: x[1]))\n","\n","\n","# extract good cases\n","cases = good_cases = [x[0] for x in temp if x[1] >= 93.5]\n","print('len(good_cases): ', len(good_cases))"],"execution_count":32,"outputs":[{"output_type":"stream","text":["len(good_cases):  7\n"],"name":"stdout"}]},{"metadata":{"id":"Doc26FeA2zpo","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":148},"outputId":"792e7eb0-a038-4e85-cef7-09f8a038e5f9","executionInfo":{"status":"ok","timestamp":1543889398414,"user_tz":-540,"elapsed":1457868,"user":{"displayName":"Seol JaeWan","photoUrl":"","userId":"07332071038155379708"}}},"cell_type":"code","source":["num_steps = 20000\n","\n","for case in cases:\n","    nn_hdim, learning_rate, batch_size, reg_lambda = case\n","\n","\n","    with tf.Session(graph=my_graph, config=conf) as my_session:\n","        tf.global_variables_initializer().run()\n","\n","        for i in range(num_steps):\n","            batch_data, batch_labels = random_sample(train_dataset, train_labels, batch_size)\n","            feed_dict = {\n","                input_data: batch_data,\n","                input_labels: batch_labels,\n","                is_training: True\n","            }\n","            train_loss, _, predictions = my_session.run([loss, optimizer, prediction], feed_dict=feed_dict)\n","\n","\n","            # print train performance(include validation performance) per 1000 steps\n","#             if i % 1000 == 0:\n","#                 train_accuracy = accuracy(predictions, batch_labels)\n","#                 print('step: %4d, train loss: %.4f, train accuracy: %.2f%%' % (i, train_loss, train_accuracy))\n","#                 valid_feed_dict = {\n","#                     input_data: valid_dataset,\n","#                     input_labels: valid_labels,\n","#                     is_training: False\n","#                 }\n","#                 valid_loss, valid_predictions = my_session.run([loss, prediction], feed_dict=valid_feed_dict)\n","#                 valid_accuracy = accuracy(valid_predictions, valid_labels)\n","#                 print('step: %4d, valid loss: %.4f, valid accuracy: %2.f%%' % (i, valid_loss, valid_accuracy))\n","\n","\n","        # test accuracy\n","        test_feed_dict = {\n","            input_data: test_dataset,\n","            input_labels: test_labels,\n","            is_training: False\n","        }\n","        test_predictions = my_session.run(prediction, feed_dict=test_feed_dict)\n","        test_accuracy = accuracy(test_predictions, test_labels)\n","        print(('case: ' + str(case)).ljust(32, ' '), 'test accuracy: %.2f%%' % (test_accuracy))\n","        result[case] = test_accuracy\n","     "],"execution_count":34,"outputs":[{"output_type":"stream","text":["case: (2048, 0.01, 512, 0.001)   test accuracy: 93.82%\n","case: (2048, 0.001, 512, 0.05)   test accuracy: 94.06%\n","case: (1024, 0.001, 512, 0.01)   test accuracy: 93.81%\n","case: (1024, 0.01, 512, 0.01)    test accuracy: 93.29%\n","case: (1024, 0.05, 512, 0.01)    test accuracy: 93.54%\n","case: (2048, 0.001, 512, 0.01)   test accuracy: 93.79%\n","case: (2048, 0.01, 512, 0.005)   test accuracy: 93.85%\n"],"name":"stdout"}]}]}